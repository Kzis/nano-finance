{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Enron Email DatasetThis dataset was collected and prepared by the CALO Project (A Cognitive Assistant that Learns and Organizes). It contains data from about 150 users, mostly senior management of Enron, organized into folders. The corpus contains a total of about 0.5M messages. This data was originally made public, and posted to the web, by the Federal Energy Regulatory Commission during its investigation.The email dataset was later purchased by Leslie Kaelbling at MIT, and turned out to have a number of integrity problems. A number of folks at SRI, notably Melinda Gervasio, worked hard to correct these problems, and it is thanks to them (not me) that the dataset is available. The dataset here does not include attachments, and some messages have been deleted \"as part of a redaction effort due to requests from affected employees\". Invalid email addresses were converted to something of the form user@enron.com whenever possible (i.e., recipient is specified in some parse-able format like \"Doe, John\" or \"Mary K. Smith\") and to no_address@enron.com when no recipient was specified.I get a number of questions about this corpus each week, which I am unable to answer, mostly because they deal with preparation issues and such that I just don\\'t know about. If you ask me a question and I don\\'t answer, please don\\'t feel slighted.I am distributing this dataset as a resource for researchers who are interested in improving current email tools, or understanding how email is currently used. This data is valuable; to my knowledge it is the only substantial collection of \"real\" email that is public. The reason other datasets are not public is because of privacy concerns. In using this dataset, please be sensitive to the privacy of the people involved (and remember that many of these people were certainly not involved in any of the actions which precipitated the investigation.)* Prior versions of the dataset are no longer being distributed. If you are using the March 2, 2004 Version; the August 21, 2009 Version; or the April 2, 2011 Version of this dataset for your work, you are requested to replace it with the newer version of the dataset below, or make the the appropriate changes to your local copy.* May 7, 2015 Version of dataset (about 423Mb, tarred and gzipped).There are also several on-line databases that allow you to search the data, at Enronemail.com, UCB, and www.enron-mail.comResearch uses of the datasetThis is a partial and poorly maintained list. If I\\'ve left your work out, don\\'t take it personally, and feel free to send me a pointer and/or description.* A paper describing the Enron data was presented at the 2004 CEAS conference.* Some experiments associated with this data are described on Ron Bekkerman\\'s home page.* A social-network analysis of the data, including \"useful mappings between the MD5 digest of the email bodies and such things as authors, recipients, etc\", is available from Andres Corrada-Emmanuel.* A group from SIMS, UC Berkeley provides search, visualization, and some email that has been labeled with topic and sentiment labels* Jitesh Shetty has put up a database of link-analysis results.* A version of the dataset with all attachments is available from EDRM.* Work at the University of Pennsylvania includes a query dataset for email search as well as a tool for generating spelling errors based on the Enron corpus.* Kimmie Farrington and colleagues published a paper in 2011 that uses the Enron dataset as part of the test corpus for their work on crowdsourcing human vs. computer generated classification explanation: see Hutton, Amanda, Alexander Liu, and Cheryl Martin. \"Crowdsourcing evaluations of classifier interpretability.\" In Proceedings of the 2012 AAAI Spring Symposium on Wisdom of the Crowd* Parakweet has released an open source set of Enron sentence data, labeled for speech acts.* A set of sentence level annotations (of what requires action or response from user) has been released by Charlie Oxborough.'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('test1.txt', 'r') as file:\n",
    "    text1 = file.read().replace('\\n', '')\n",
    "text1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Enron scandalThe Enron scandal, publicized in October 2001, eventually led to the bankruptcy of the Enron Corporation, an American energy company based in Houston, Texas, and the de facto dissolution of Arthur Andersen, which was one of the five largest audit and accountancy partnerships in the world. In addition to being the largest bankruptcy reorganization in American history at that time, Enron was cited as the biggest audit failure.[1]:61Enron was formed in 1985 by Kenneth Lay after merging Houston Natural Gas and InterNorth. Several years later, when Jeffrey Skilling was hired, he developed a staff of executives that – by the use of accounting loopholes, special purpose entities, and poor financial reporting – were able to hide billions of dollars in debt from failed deals and projects. Chief Financial Officer Andrew Fastow and other executives not only misled Enron's Board of Directors and Audit Committee on high-risk accounting practices, but also pressured Arthur Andersen to ignore the issues.Enron shareholders filed a $40 billion lawsuit after the company's stock price, which achieved a high of US$90.75 per share in mid-2000, plummeted to less than $1 by the end of November 2001.[2] The U.S. Securities and Exchange Commission (SEC) began an investigation, and rival Houston competitor Dynegy offered to purchase the company at a very low price. The deal failed, and on December 2, 2001, Enron filed for bankruptcy under Chapter 11 of the United States Bankruptcy Code. Enron's $63.4 billion in assets made it the largest corporate bankruptcy in U.S. history until WorldCom's bankruptcy the next year.[3]Many executives at Enron were indicted for a variety of charges and some were later sentenced to prison. Andersen was found guilty of illegally destroying documents relevant to the SEC investigation, which voided its license to audit public companies and effectively closed the firm. By the time the ruling was overturned at the U.S. Supreme Court, the company had lost the majority of its customers and had ceased operating. Enron employees and shareholders received limited returns in lawsuits, despite losing billions in pensions and stock prices.As a consequence of the scandal, new regulations and legislation were enacted to expand the accuracy of financial reporting for public companies.[4] One piece of legislation, the Sarbanes–Oxley Act, increased penalties for destroying, altering, or fabricating records in federal investigations or for attempting to defraud shareholders.[5] The act also increased the accountability of auditing firms to remain unbiased and independent of their clients.[4]\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('test2.txt', 'r') as file:\n",
    "    text2 = file.read().replace('\\n', '')\n",
    "text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5hL8ub5U8NjI"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Enron',\n",
       " 'nron ',\n",
       " 'ron E',\n",
       " 'on Em',\n",
       " 'n Ema',\n",
       " ' Emai',\n",
       " 'Email',\n",
       " 'mail ',\n",
       " 'ail D',\n",
       " 'il Da',\n",
       " 'l Dat',\n",
       " ' Data',\n",
       " 'Datas',\n",
       " 'atase',\n",
       " 'taset',\n",
       " 'asetT',\n",
       " 'setTh',\n",
       " 'etThi',\n",
       " 'tThis',\n",
       " 'This ',\n",
       " 'his d',\n",
       " 'is da',\n",
       " 's dat',\n",
       " ' data',\n",
       " 'datas',\n",
       " 'atase',\n",
       " 'taset',\n",
       " 'aset ',\n",
       " 'set w',\n",
       " 'et wa',\n",
       " 't was',\n",
       " ' was ',\n",
       " 'was c',\n",
       " 'as co',\n",
       " 's col',\n",
       " ' coll',\n",
       " 'colle',\n",
       " 'ollec',\n",
       " 'llect',\n",
       " 'lecte',\n",
       " 'ected',\n",
       " 'cted ',\n",
       " 'ted a',\n",
       " 'ed an',\n",
       " 'd and',\n",
       " ' and ',\n",
       " 'and p',\n",
       " 'nd pr',\n",
       " 'd pre',\n",
       " ' prep',\n",
       " 'prepa',\n",
       " 'repar',\n",
       " 'epare',\n",
       " 'pared',\n",
       " 'ared ',\n",
       " 'red b',\n",
       " 'ed by',\n",
       " 'd by ',\n",
       " ' by t',\n",
       " 'by th',\n",
       " 'y the',\n",
       " ' the ',\n",
       " 'the C',\n",
       " 'he CA',\n",
       " 'e CAL',\n",
       " ' CALO',\n",
       " 'CALO ',\n",
       " 'ALO P',\n",
       " 'LO Pr',\n",
       " 'O Pro',\n",
       " ' Proj',\n",
       " 'Proje',\n",
       " 'rojec',\n",
       " 'oject',\n",
       " 'ject ',\n",
       " 'ect (',\n",
       " 'ct (A',\n",
       " 't (A ',\n",
       " ' (A C',\n",
       " '(A Co',\n",
       " 'A Cog',\n",
       " ' Cogn',\n",
       " 'Cogni',\n",
       " 'ognit',\n",
       " 'gniti',\n",
       " 'nitiv',\n",
       " 'itive',\n",
       " 'tive ',\n",
       " 'ive A',\n",
       " 've As',\n",
       " 'e Ass',\n",
       " ' Assi',\n",
       " 'Assis',\n",
       " 'ssist',\n",
       " 'sista',\n",
       " 'istan',\n",
       " 'stant',\n",
       " 'tant ',\n",
       " 'ant t',\n",
       " 'nt th',\n",
       " 't tha',\n",
       " ' that',\n",
       " 'that ',\n",
       " 'hat L',\n",
       " 'at Le',\n",
       " 't Lea',\n",
       " ' Lear',\n",
       " 'Learn',\n",
       " 'earns',\n",
       " 'arns ',\n",
       " 'rns a',\n",
       " 'ns an',\n",
       " 's and',\n",
       " ' and ',\n",
       " 'and O',\n",
       " 'nd Or',\n",
       " 'd Org',\n",
       " ' Orga',\n",
       " 'Organ',\n",
       " 'rgani',\n",
       " 'ganiz',\n",
       " 'anize',\n",
       " 'nizes',\n",
       " 'izes)',\n",
       " 'zes).',\n",
       " 'es). ',\n",
       " 's). I',\n",
       " '). It',\n",
       " '. It ',\n",
       " ' It c',\n",
       " 'It co',\n",
       " 't con',\n",
       " ' cont',\n",
       " 'conta',\n",
       " 'ontai',\n",
       " 'ntain',\n",
       " 'tains',\n",
       " 'ains ',\n",
       " 'ins d',\n",
       " 'ns da',\n",
       " 's dat',\n",
       " ' data',\n",
       " 'data ',\n",
       " 'ata f',\n",
       " 'ta fr',\n",
       " 'a fro',\n",
       " ' from',\n",
       " 'from ',\n",
       " 'rom a',\n",
       " 'om ab',\n",
       " 'm abo',\n",
       " ' abou',\n",
       " 'about',\n",
       " 'bout ',\n",
       " 'out 1',\n",
       " 'ut 15',\n",
       " 't 150',\n",
       " ' 150 ',\n",
       " '150 u',\n",
       " '50 us',\n",
       " '0 use',\n",
       " ' user',\n",
       " 'users',\n",
       " 'sers,',\n",
       " 'ers, ',\n",
       " 'rs, m',\n",
       " 's, mo',\n",
       " ', mos',\n",
       " ' most',\n",
       " 'mostl',\n",
       " 'ostly',\n",
       " 'stly ',\n",
       " 'tly s',\n",
       " 'ly se',\n",
       " 'y sen',\n",
       " ' seni',\n",
       " 'senio',\n",
       " 'enior',\n",
       " 'nior ',\n",
       " 'ior m',\n",
       " 'or ma',\n",
       " 'r man',\n",
       " ' mana',\n",
       " 'manag',\n",
       " 'anage',\n",
       " 'nagem',\n",
       " 'ageme',\n",
       " 'gemen',\n",
       " 'ement',\n",
       " 'ment ',\n",
       " 'ent o',\n",
       " 'nt of',\n",
       " 't of ',\n",
       " ' of E',\n",
       " 'of En',\n",
       " 'f Enr',\n",
       " ' Enro',\n",
       " 'Enron',\n",
       " 'nron,',\n",
       " 'ron, ',\n",
       " 'on, o',\n",
       " 'n, or',\n",
       " ', org',\n",
       " ' orga',\n",
       " 'organ',\n",
       " 'rgani',\n",
       " 'ganiz',\n",
       " 'anize',\n",
       " 'nized',\n",
       " 'ized ',\n",
       " 'zed i',\n",
       " 'ed in',\n",
       " 'd int',\n",
       " ' into',\n",
       " 'into ',\n",
       " 'nto f',\n",
       " 'to fo',\n",
       " 'o fol',\n",
       " ' fold',\n",
       " 'folde',\n",
       " 'older',\n",
       " 'lders',\n",
       " 'ders.',\n",
       " 'ers. ',\n",
       " 'rs. T',\n",
       " 's. Th',\n",
       " '. The',\n",
       " ' The ',\n",
       " 'The c',\n",
       " 'he co',\n",
       " 'e cor',\n",
       " ' corp',\n",
       " 'corpu',\n",
       " 'orpus',\n",
       " 'rpus ',\n",
       " 'pus c',\n",
       " 'us co',\n",
       " 's con',\n",
       " ' cont',\n",
       " 'conta',\n",
       " 'ontai',\n",
       " 'ntain',\n",
       " 'tains',\n",
       " 'ains ',\n",
       " 'ins a',\n",
       " 'ns a ',\n",
       " 's a t',\n",
       " ' a to',\n",
       " 'a tot',\n",
       " ' tota',\n",
       " 'total',\n",
       " 'otal ',\n",
       " 'tal o',\n",
       " 'al of',\n",
       " 'l of ',\n",
       " ' of a',\n",
       " 'of ab',\n",
       " 'f abo',\n",
       " ' abou',\n",
       " 'about',\n",
       " 'bout ',\n",
       " 'out 0',\n",
       " 'ut 0.',\n",
       " 't 0.5',\n",
       " ' 0.5M',\n",
       " '0.5M ',\n",
       " '.5M m',\n",
       " '5M me',\n",
       " 'M mes',\n",
       " ' mess',\n",
       " 'messa',\n",
       " 'essag',\n",
       " 'ssage',\n",
       " 'sages',\n",
       " 'ages.',\n",
       " 'ges. ',\n",
       " 'es. T',\n",
       " 's. Th',\n",
       " '. Thi',\n",
       " ' This',\n",
       " 'This ',\n",
       " 'his d',\n",
       " 'is da',\n",
       " 's dat',\n",
       " ' data',\n",
       " 'data ',\n",
       " 'ata w',\n",
       " 'ta wa',\n",
       " 'a was',\n",
       " ' was ',\n",
       " 'was o',\n",
       " 'as or',\n",
       " 's ori',\n",
       " ' orig',\n",
       " 'origi',\n",
       " 'rigin',\n",
       " 'igina',\n",
       " 'ginal',\n",
       " 'inall',\n",
       " 'nally',\n",
       " 'ally ',\n",
       " 'lly m',\n",
       " 'ly ma',\n",
       " 'y mad',\n",
       " ' made',\n",
       " 'made ',\n",
       " 'ade p',\n",
       " 'de pu',\n",
       " 'e pub',\n",
       " ' publ',\n",
       " 'publi',\n",
       " 'ublic',\n",
       " 'blic,',\n",
       " 'lic, ',\n",
       " 'ic, a',\n",
       " 'c, an',\n",
       " ', and',\n",
       " ' and ',\n",
       " 'and p',\n",
       " 'nd po',\n",
       " 'd pos',\n",
       " ' post',\n",
       " 'poste',\n",
       " 'osted',\n",
       " 'sted ',\n",
       " 'ted t',\n",
       " 'ed to',\n",
       " 'd to ',\n",
       " ' to t',\n",
       " 'to th',\n",
       " 'o the',\n",
       " ' the ',\n",
       " 'the w',\n",
       " 'he we',\n",
       " 'e web',\n",
       " ' web,',\n",
       " 'web, ',\n",
       " 'eb, b',\n",
       " 'b, by',\n",
       " ', by ',\n",
       " ' by t',\n",
       " 'by th',\n",
       " 'y the',\n",
       " ' the ',\n",
       " 'the F',\n",
       " 'he Fe',\n",
       " 'e Fed',\n",
       " ' Fede',\n",
       " 'Feder',\n",
       " 'edera',\n",
       " 'deral',\n",
       " 'eral ',\n",
       " 'ral E',\n",
       " 'al En',\n",
       " 'l Ene',\n",
       " ' Ener',\n",
       " 'Energ',\n",
       " 'nergy',\n",
       " 'ergy ',\n",
       " 'rgy R',\n",
       " 'gy Re',\n",
       " 'y Reg',\n",
       " ' Regu',\n",
       " 'Regul',\n",
       " 'egula',\n",
       " 'gulat',\n",
       " 'ulato',\n",
       " 'lator',\n",
       " 'atory',\n",
       " 'tory ',\n",
       " 'ory C',\n",
       " 'ry Co',\n",
       " 'y Com',\n",
       " ' Comm',\n",
       " 'Commi',\n",
       " 'ommis',\n",
       " 'mmiss',\n",
       " 'missi',\n",
       " 'issio',\n",
       " 'ssion',\n",
       " 'sion ',\n",
       " 'ion d',\n",
       " 'on du',\n",
       " 'n dur',\n",
       " ' duri',\n",
       " 'durin',\n",
       " 'uring',\n",
       " 'ring ',\n",
       " 'ing i',\n",
       " 'ng it',\n",
       " 'g its',\n",
       " ' its ',\n",
       " 'its i',\n",
       " 'ts in',\n",
       " 's inv',\n",
       " ' inve',\n",
       " 'inves',\n",
       " 'nvest',\n",
       " 'vesti',\n",
       " 'estig',\n",
       " 'stiga',\n",
       " 'tigat',\n",
       " 'igati',\n",
       " 'gatio',\n",
       " 'ation',\n",
       " 'tion.',\n",
       " 'ion.T',\n",
       " 'on.Th',\n",
       " 'n.The',\n",
       " '.The ',\n",
       " 'The e',\n",
       " 'he em',\n",
       " 'e ema',\n",
       " ' emai',\n",
       " 'email',\n",
       " 'mail ',\n",
       " 'ail d',\n",
       " 'il da',\n",
       " 'l dat',\n",
       " ' data',\n",
       " 'datas',\n",
       " 'atase',\n",
       " 'taset',\n",
       " 'aset ',\n",
       " 'set w',\n",
       " 'et wa',\n",
       " 't was',\n",
       " ' was ',\n",
       " 'was l',\n",
       " 'as la',\n",
       " 's lat',\n",
       " ' late',\n",
       " 'later',\n",
       " 'ater ',\n",
       " 'ter p',\n",
       " 'er pu',\n",
       " 'r pur',\n",
       " ' purc',\n",
       " 'purch',\n",
       " 'urcha',\n",
       " 'rchas',\n",
       " 'chase',\n",
       " 'hased',\n",
       " 'ased ',\n",
       " 'sed b',\n",
       " 'ed by',\n",
       " 'd by ',\n",
       " ' by L',\n",
       " 'by Le',\n",
       " 'y Les',\n",
       " ' Lesl',\n",
       " 'Lesli',\n",
       " 'eslie',\n",
       " 'slie ',\n",
       " 'lie K',\n",
       " 'ie Ka',\n",
       " 'e Kae',\n",
       " ' Kael',\n",
       " 'Kaelb',\n",
       " 'aelbl',\n",
       " 'elbli',\n",
       " 'lblin',\n",
       " 'bling',\n",
       " 'ling ',\n",
       " 'ing a',\n",
       " 'ng at',\n",
       " 'g at ',\n",
       " ' at M',\n",
       " 'at MI',\n",
       " 't MIT',\n",
       " ' MIT,',\n",
       " 'MIT, ',\n",
       " 'IT, a',\n",
       " 'T, an',\n",
       " ', and',\n",
       " ' and ',\n",
       " 'and t',\n",
       " 'nd tu',\n",
       " 'd tur',\n",
       " ' turn',\n",
       " 'turne',\n",
       " 'urned',\n",
       " 'rned ',\n",
       " 'ned o',\n",
       " 'ed ou',\n",
       " 'd out',\n",
       " ' out ',\n",
       " 'out t',\n",
       " 'ut to',\n",
       " 't to ',\n",
       " ' to h',\n",
       " 'to ha',\n",
       " 'o hav',\n",
       " ' have',\n",
       " 'have ',\n",
       " 'ave a',\n",
       " 've a ',\n",
       " 'e a n',\n",
       " ' a nu',\n",
       " 'a num',\n",
       " ' numb',\n",
       " 'numbe',\n",
       " 'umber',\n",
       " 'mber ',\n",
       " 'ber o',\n",
       " 'er of',\n",
       " 'r of ',\n",
       " ' of i',\n",
       " 'of in',\n",
       " 'f int',\n",
       " ' inte',\n",
       " 'integ',\n",
       " 'ntegr',\n",
       " 'tegri',\n",
       " 'egrit',\n",
       " 'grity',\n",
       " 'rity ',\n",
       " 'ity p',\n",
       " 'ty pr',\n",
       " 'y pro',\n",
       " ' prob',\n",
       " 'probl',\n",
       " 'roble',\n",
       " 'oblem',\n",
       " 'blems',\n",
       " 'lems.',\n",
       " 'ems. ',\n",
       " 'ms. A',\n",
       " 's. A ',\n",
       " '. A n',\n",
       " ' A nu',\n",
       " 'A num',\n",
       " ' numb',\n",
       " 'numbe',\n",
       " 'umber',\n",
       " 'mber ',\n",
       " 'ber o',\n",
       " 'er of',\n",
       " 'r of ',\n",
       " ' of f',\n",
       " 'of fo',\n",
       " 'f fol',\n",
       " ' folk',\n",
       " 'folks',\n",
       " 'olks ',\n",
       " 'lks a',\n",
       " 'ks at',\n",
       " 's at ',\n",
       " ' at S',\n",
       " 'at SR',\n",
       " 't SRI',\n",
       " ' SRI,',\n",
       " 'SRI, ',\n",
       " 'RI, n',\n",
       " 'I, no',\n",
       " ', not',\n",
       " ' nota',\n",
       " 'notab',\n",
       " 'otabl',\n",
       " 'tably',\n",
       " 'ably ',\n",
       " 'bly M',\n",
       " 'ly Me',\n",
       " 'y Mel',\n",
       " ' Meli',\n",
       " 'Melin',\n",
       " 'elind',\n",
       " 'linda',\n",
       " 'inda ',\n",
       " 'nda G',\n",
       " 'da Ge',\n",
       " 'a Ger',\n",
       " ' Gerv',\n",
       " 'Gerva',\n",
       " 'ervas',\n",
       " 'rvasi',\n",
       " 'vasio',\n",
       " 'asio,',\n",
       " 'sio, ',\n",
       " 'io, w',\n",
       " 'o, wo',\n",
       " ', wor',\n",
       " ' work',\n",
       " 'worke',\n",
       " 'orked',\n",
       " 'rked ',\n",
       " 'ked h',\n",
       " 'ed ha',\n",
       " 'd har',\n",
       " ' hard',\n",
       " 'hard ',\n",
       " 'ard t',\n",
       " 'rd to',\n",
       " 'd to ',\n",
       " ' to c',\n",
       " 'to co',\n",
       " 'o cor',\n",
       " ' corr',\n",
       " 'corre',\n",
       " 'orrec',\n",
       " 'rrect',\n",
       " 'rect ',\n",
       " 'ect t',\n",
       " 'ct th',\n",
       " 't the',\n",
       " ' thes',\n",
       " 'these',\n",
       " 'hese ',\n",
       " 'ese p',\n",
       " 'se pr',\n",
       " 'e pro',\n",
       " ' prob',\n",
       " 'probl',\n",
       " 'roble',\n",
       " 'oblem',\n",
       " 'blems',\n",
       " 'lems,',\n",
       " 'ems, ',\n",
       " 'ms, a',\n",
       " 's, an',\n",
       " ', and',\n",
       " ' and ',\n",
       " 'and i',\n",
       " 'nd it',\n",
       " 'd it ',\n",
       " ' it i',\n",
       " 'it is',\n",
       " 't is ',\n",
       " ' is t',\n",
       " 'is th',\n",
       " 's tha',\n",
       " ' than',\n",
       " 'thank',\n",
       " 'hanks',\n",
       " 'anks ',\n",
       " 'nks t',\n",
       " 'ks to',\n",
       " 's to ',\n",
       " ' to t',\n",
       " 'to th',\n",
       " 'o the',\n",
       " ' them',\n",
       " 'them ',\n",
       " 'hem (',\n",
       " 'em (n',\n",
       " 'm (no',\n",
       " ' (not',\n",
       " '(not ',\n",
       " 'not m',\n",
       " 'ot me',\n",
       " 't me)',\n",
       " ' me) ',\n",
       " 'me) t',\n",
       " 'e) th',\n",
       " ') tha',\n",
       " ' that',\n",
       " 'that ',\n",
       " 'hat t',\n",
       " 'at th',\n",
       " 't the',\n",
       " ' the ',\n",
       " 'the d',\n",
       " 'he da',\n",
       " 'e dat',\n",
       " ' data',\n",
       " 'datas',\n",
       " 'atase',\n",
       " 'taset',\n",
       " 'aset ',\n",
       " 'set i',\n",
       " 'et is',\n",
       " 't is ',\n",
       " ' is a',\n",
       " 'is av',\n",
       " 's ava',\n",
       " ' avai',\n",
       " 'avail',\n",
       " 'vaila',\n",
       " 'ailab',\n",
       " 'ilabl',\n",
       " 'lable',\n",
       " 'able.',\n",
       " 'ble. ',\n",
       " 'le. T',\n",
       " 'e. Th',\n",
       " '. The',\n",
       " ' The ',\n",
       " 'The d',\n",
       " 'he da',\n",
       " 'e dat',\n",
       " ' data',\n",
       " 'datas',\n",
       " 'atase',\n",
       " 'taset',\n",
       " 'aset ',\n",
       " 'set h',\n",
       " 'et he',\n",
       " 't her',\n",
       " ' here',\n",
       " 'here ',\n",
       " 'ere d',\n",
       " 're do',\n",
       " 'e doe',\n",
       " ' does',\n",
       " 'does ',\n",
       " 'oes n',\n",
       " 'es no',\n",
       " 's not',\n",
       " ' not ',\n",
       " 'not i',\n",
       " 'ot in',\n",
       " 't inc',\n",
       " ' incl',\n",
       " 'inclu',\n",
       " 'nclud',\n",
       " 'clude',\n",
       " 'lude ',\n",
       " 'ude a',\n",
       " 'de at',\n",
       " 'e att',\n",
       " ' atta',\n",
       " 'attac',\n",
       " 'ttach',\n",
       " 'tachm',\n",
       " 'achme',\n",
       " 'chmen',\n",
       " 'hment',\n",
       " 'ments',\n",
       " 'ents,',\n",
       " 'nts, ',\n",
       " 'ts, a',\n",
       " 's, an',\n",
       " ', and',\n",
       " ' and ',\n",
       " 'and s',\n",
       " 'nd so',\n",
       " 'd som',\n",
       " ' some',\n",
       " 'some ',\n",
       " 'ome m',\n",
       " 'me me',\n",
       " 'e mes',\n",
       " ' mess',\n",
       " 'messa',\n",
       " 'essag',\n",
       " 'ssage',\n",
       " 'sages',\n",
       " 'ages ',\n",
       " 'ges h',\n",
       " 'es ha',\n",
       " 's hav',\n",
       " ' have',\n",
       " 'have ',\n",
       " 'ave b',\n",
       " 've be',\n",
       " 'e bee',\n",
       " ' been',\n",
       " 'been ',\n",
       " 'een d',\n",
       " 'en de',\n",
       " 'n del',\n",
       " ' dele',\n",
       " 'delet',\n",
       " 'elete',\n",
       " 'leted',\n",
       " 'eted ',\n",
       " 'ted \"',\n",
       " 'ed \"a',\n",
       " 'd \"as',\n",
       " ' \"as ',\n",
       " '\"as p',\n",
       " 'as pa',\n",
       " 's par',\n",
       " ' part',\n",
       " 'part ',\n",
       " 'art o',\n",
       " 'rt of',\n",
       " 't of ',\n",
       " ' of a',\n",
       " 'of a ',\n",
       " 'f a r',\n",
       " ' a re',\n",
       " 'a red',\n",
       " ' reda',\n",
       " 'redac',\n",
       " 'edact',\n",
       " 'dacti',\n",
       " 'actio',\n",
       " 'ction',\n",
       " 'tion ',\n",
       " 'ion e',\n",
       " 'on ef',\n",
       " 'n eff',\n",
       " ' effo',\n",
       " 'effor',\n",
       " 'ffort',\n",
       " 'fort ',\n",
       " 'ort d',\n",
       " 'rt du',\n",
       " 't due',\n",
       " ' due ',\n",
       " 'due t',\n",
       " 'ue to',\n",
       " 'e to ',\n",
       " ' to r',\n",
       " 'to re',\n",
       " 'o req',\n",
       " ' requ',\n",
       " 'reque',\n",
       " 'eques',\n",
       " 'quest',\n",
       " 'uests',\n",
       " 'ests ',\n",
       " 'sts f',\n",
       " 'ts fr',\n",
       " 's fro',\n",
       " ' from',\n",
       " 'from ',\n",
       " 'rom a',\n",
       " 'om af',\n",
       " 'm aff',\n",
       " ' affe',\n",
       " 'affec',\n",
       " 'ffect',\n",
       " 'fecte',\n",
       " 'ected',\n",
       " 'cted ',\n",
       " 'ted e',\n",
       " 'ed em',\n",
       " 'd emp',\n",
       " ' empl',\n",
       " 'emplo',\n",
       " 'mploy',\n",
       " 'ploye',\n",
       " 'loyee',\n",
       " 'oyees',\n",
       " 'yees\"',\n",
       " 'ees\".',\n",
       " 'es\". ',\n",
       " 's\". I',\n",
       " '\". In',\n",
       " '. Inv',\n",
       " ' Inva',\n",
       " 'Inval',\n",
       " 'nvali',\n",
       " 'valid',\n",
       " 'alid ',\n",
       " 'lid e',\n",
       " 'id em',\n",
       " 'd ema',\n",
       " ' emai',\n",
       " 'email',\n",
       " 'mail ',\n",
       " 'ail a',\n",
       " 'il ad',\n",
       " 'l add',\n",
       " ' addr',\n",
       " 'addre',\n",
       " 'ddres',\n",
       " 'dress',\n",
       " 'resse',\n",
       " 'esses',\n",
       " 'sses ',\n",
       " 'ses w',\n",
       " 'es we',\n",
       " 's wer',\n",
       " ' were',\n",
       " 'were ',\n",
       " 'ere c',\n",
       " 're co',\n",
       " 'e con',\n",
       " ' conv',\n",
       " 'conve',\n",
       " 'onver',\n",
       " 'nvert',\n",
       " 'verte',\n",
       " 'erted',\n",
       " 'rted ',\n",
       " 'ted t',\n",
       " 'ed to',\n",
       " 'd to ',\n",
       " ' to s',\n",
       " 'to so',\n",
       " 'o som',\n",
       " ' some',\n",
       " 'somet',\n",
       " 'ometh',\n",
       " 'methi',\n",
       " 'ethin',\n",
       " 'thing',\n",
       " 'hing ',\n",
       " 'ing o',\n",
       " 'ng of',\n",
       " 'g of ',\n",
       " ' of t',\n",
       " 'of th',\n",
       " 'f the',\n",
       " ' the ',\n",
       " 'the f',\n",
       " 'he fo',\n",
       " 'e for',\n",
       " ' form',\n",
       " 'form ',\n",
       " 'orm u',\n",
       " 'rm us',\n",
       " 'm use',\n",
       " ' user',\n",
       " 'user@',\n",
       " 'ser@e',\n",
       " 'er@en',\n",
       " 'r@enr',\n",
       " '@enro',\n",
       " 'enron',\n",
       " 'nron.',\n",
       " 'ron.c',\n",
       " 'on.co',\n",
       " 'n.com',\n",
       " '.com ',\n",
       " 'com w',\n",
       " 'om wh',\n",
       " 'm whe',\n",
       " ' when',\n",
       " 'whene',\n",
       " 'henev',\n",
       " 'eneve',\n",
       " 'never',\n",
       " 'ever ',\n",
       " 'ver p',\n",
       " 'er po',\n",
       " 'r pos',\n",
       " ' poss',\n",
       " 'possi',\n",
       " 'ossib',\n",
       " 'ssibl',\n",
       " 'sible',\n",
       " 'ible ',\n",
       " 'ble (',\n",
       " 'le (i',\n",
       " 'e (i.',\n",
       " ' (i.e',\n",
       " '(i.e.',\n",
       " 'i.e.,',\n",
       " '.e., ',\n",
       " 'e., r',\n",
       " '., re',\n",
       " ', rec',\n",
       " ' reci',\n",
       " 'recip',\n",
       " 'ecipi',\n",
       " 'cipie',\n",
       " 'ipien',\n",
       " 'pient',\n",
       " 'ient ',\n",
       " 'ent i',\n",
       " 'nt is',\n",
       " 't is ',\n",
       " ' is s',\n",
       " 'is sp',\n",
       " 's spe',\n",
       " ' spec',\n",
       " 'speci',\n",
       " 'pecif',\n",
       " 'ecifi',\n",
       " 'cifie',\n",
       " 'ified',\n",
       " 'fied ',\n",
       " 'ied i',\n",
       " 'ed in',\n",
       " 'd in ',\n",
       " ' in s',\n",
       " 'in so',\n",
       " 'n som',\n",
       " ' some',\n",
       " 'some ',\n",
       " 'ome p',\n",
       " 'me pa',\n",
       " 'e par',\n",
       " ' pars',\n",
       " 'parse',\n",
       " 'arse-',\n",
       " 'rse-a',\n",
       " 'se-ab',\n",
       " 'e-abl',\n",
       " '-able',\n",
       " 'able ',\n",
       " 'ble f',\n",
       " 'le fo',\n",
       " 'e for',\n",
       " ' form',\n",
       " 'forma',\n",
       " 'ormat',\n",
       " 'rmat ',\n",
       " 'mat l',\n",
       " 'at li',\n",
       " 't lik',\n",
       " ' like',\n",
       " 'like ',\n",
       " 'ike \"',\n",
       " 'ke \"D',\n",
       " 'e \"Do',\n",
       " ...]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# document = 'Lorem Ipsum dolor sit amet'\n",
    "# shingle and discard the last 5 as these are just the last n<5 characters from the document\n",
    "\n",
    "document = text1\n",
    "shingles = [document[i:i+5] for i in range(len(document))][:-5]\n",
    "shingles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aVnhekjj8WlJ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06913740788903337"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# other_document = 'Lorem Ipsum dolor sit amet is how dummy text starts'\n",
    "# shingle and discard the last 5 as these are just the last n<5 characters from the document\n",
    "\n",
    "other_document = text2\n",
    "other_shingles = [other_document[i:i+5] for i in range(len(other_document))][:-5]\n",
    "\n",
    "# Jaccard distance is the size of set intersection divided by the size of set union\n",
    "len(set(shingles) & set(other_shingles)) / len(set(shingles) | set(other_shingles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasketch\n",
      "  Downloading https://files.pythonhosted.org/packages/27/bb/461ed0bb4097a3ef10ccaab3c889a39767a42aca496c2ed5d946c5ab8739/datasketch-1.4.7-py2.py3-none-any.whl (66kB)\n",
      "Collecting redis>=2.10.0 (from datasketch)\n",
      "  Downloading https://files.pythonhosted.org/packages/bd/64/b1e90af9bf0c7f6ef55e46b81ab527b33b785824d65300bb65636534b530/redis-3.3.8-py2.py3-none-any.whl (66kB)\n",
      "Requirement already satisfied: numpy>=1.11 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from datasketch) (1.14.5)\n",
      "Installing collected packages: redis, datasketch\n",
      "Successfully installed datasketch-1.4.7 redis-3.3.8\n"
     ]
    }
   ],
   "source": [
    "!pip install datasketch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vszDr2SE9nUy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate neighbours with Jaccard similarity > 0.5 []\n"
     ]
    }
   ],
   "source": [
    "from datasketch import MinHash, MinHashLSH\n",
    "\n",
    "set1 = set(shingles)\n",
    "set2 = set(other_shingles)\n",
    "\n",
    "m1 = MinHash(num_perm=128)\n",
    "m2 = MinHash(num_perm=128)\n",
    "\n",
    "for d in set1:\n",
    "    m1.update(d.encode('utf8'))\n",
    "for d in set2:\n",
    "    m2.update(d.encode('utf8'))\n",
    "\n",
    "\n",
    "# Create LSH index\n",
    "lsh = MinHashLSH(threshold=0.5, num_perm=128)\n",
    "lsh.insert(\"m2\", m2)\n",
    "\n",
    "result = lsh.query(m1)\n",
    "print(\"Approximate neighbours with Jaccard similarity > 0.5\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "6gGdxz5U_aSd",
    "outputId": "8e755891-6c9b-4a85-e532-1f1d0c6f1cfa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate neighbours with Jaccard similarity > 0.5 ['m3', 'm2']\n"
     ]
    }
   ],
   "source": [
    "from datasketch import MinHash, MinHashLSH\n",
    "\n",
    "set1 = set(['minhash', 'is', 'a', 'probabilistic', 'data', 'structure', 'for',\n",
    "            'estimating', 'the', 'similarity', 'between', 'datasets'])\n",
    "set2 = set(['minhash', 'is', 'a', 'probability', 'data', 'structure', 'for',\n",
    "            'estimating', 'the', 'similarity', 'between', 'documents'])\n",
    "set3 = set(['minhash', 'is', 'probability', 'data', 'structure', 'for',\n",
    "            'estimating', 'the', 'similarity', 'between', 'documents'])\n",
    "\n",
    "m1 = MinHash(num_perm=128)\n",
    "m2 = MinHash(num_perm=128)\n",
    "m3 = MinHash(num_perm=128)\n",
    "for d in set1:\n",
    "    m1.update(d.encode('utf8'))\n",
    "for d in set2:\n",
    "    m2.update(d.encode('utf8'))\n",
    "for d in set3:\n",
    "    m3.update(d.encode('utf8'))\n",
    "\n",
    "# Create LSH index\n",
    "lsh = MinHashLSH(threshold=0.5, num_perm=128)\n",
    "lsh.insert(\"m2\", m2)\n",
    "lsh.insert(\"m3\", m3)\n",
    "result = lsh.query(m1)\n",
    "print(\"Approximate neighbours with Jaccard similarity > 0.5\", result)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "LSH.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

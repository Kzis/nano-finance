{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R0TRvmdp21pX"
   },
   "source": [
    "### Min hashing and Jaccard similarity\n",
    "\n",
    "#### Introduction\n",
    "\n",
    "There are many ways to measure how similar two strings are: [Hamming distance](http://nbviewer.ipython.org/github/BenLangmead/comp-genomics-class/blob/master/notebooks/CG_DP_EditDist.ipynb), [edit distance](http://nbviewer.ipython.org/github/BenLangmead/comp-genomics-class/blob/master/notebooks/CG_DP_EditDist.ipynb) and [global alignment value](http://nbviewer.ipython.org/github/BenLangmead/comp-genomics-class/blob/master/notebooks/CG_DP_Global.ipynb) for example.  Another way is to turn each string into a set, e.g. the set of its constituent $k$-mers, then consider how similar the sets are.\n",
    "\n",
    "We define a function that, given a string, returns the set of its constituent $k$-mers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Enron Email DatasetThis dataset was collected and prepared by the CALO Project (A Cognitive Assistant that Learns and Organizes). It contains data from about 150 users, mostly senior management of Enron, organized into folders. The corpus contains a total of about 0.5M messages. This data was originally made public, and posted to the web, by the Federal Energy Regulatory Commission during its investigation.The email dataset was later purchased by Leslie Kaelbling at MIT, and turned out to have a number of integrity problems. A number of folks at SRI, notably Melinda Gervasio, worked hard to correct these problems, and it is thanks to them (not me) that the dataset is available. The dataset here does not include attachments, and some messages have been deleted \"as part of a redaction effort due to requests from affected employees\". Invalid email addresses were converted to something of the form user@enron.com whenever possible (i.e., recipient is specified in some parse-able format like \"Doe, John\" or \"Mary K. Smith\") and to no_address@enron.com when no recipient was specified.I get a number of questions about this corpus each week, which I am unable to answer, mostly because they deal with preparation issues and such that I just don\\'t know about. If you ask me a question and I don\\'t answer, please don\\'t feel slighted.I am distributing this dataset as a resource for researchers who are interested in improving current email tools, or understanding how email is currently used. This data is valuable; to my knowledge it is the only substantial collection of \"real\" email that is public. The reason other datasets are not public is because of privacy concerns. In using this dataset, please be sensitive to the privacy of the people involved (and remember that many of these people were certainly not involved in any of the actions which precipitated the investigation.)* Prior versions of the dataset are no longer being distributed. If you are using the March 2, 2004 Version; the August 21, 2009 Version; or the April 2, 2011 Version of this dataset for your work, you are requested to replace it with the newer version of the dataset below, or make the the appropriate changes to your local copy.* May 7, 2015 Version of dataset (about 423Mb, tarred and gzipped).There are also several on-line databases that allow you to search the data, at Enronemail.com, UCB, and www.enron-mail.comResearch uses of the datasetThis is a partial and poorly maintained list. If I\\'ve left your work out, don\\'t take it personally, and feel free to send me a pointer and/or description.* A paper describing the Enron data was presented at the 2004 CEAS conference.* Some experiments associated with this data are described on Ron Bekkerman\\'s home page.* A social-network analysis of the data, including \"useful mappings between the MD5 digest of the email bodies and such things as authors, recipients, etc\", is available from Andres Corrada-Emmanuel.* A group from SIMS, UC Berkeley provides search, visualization, and some email that has been labeled with topic and sentiment labels* Jitesh Shetty has put up a database of link-analysis results.* A version of the dataset with all attachments is available from EDRM.* Work at the University of Pennsylvania includes a query dataset for email search as well as a tool for generating spelling errors based on the Enron corpus.* Kimmie Farrington and colleagues published a paper in 2011 that uses the Enron dataset as part of the test corpus for their work on crowdsourcing human vs. computer generated classification explanation: see Hutton, Amanda, Alexander Liu, and Cheryl Martin. \"Crowdsourcing evaluations of classifier interpretability.\" In Proceedings of the 2012 AAAI Spring Symposium on Wisdom of the Crowd* Parakweet has released an open source set of Enron sentence data, labeled for speech acts.* A set of sentence level annotations (of what requires action or response from user) has been released by Charlie Oxborough.'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('test1.txt', 'r') as file:\n",
    "    text1 = file.read().replace('\\n', '')\n",
    "text1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Enron scandalThe Enron scandal, publicized in October 2001, eventually led to the bankruptcy of the Enron Corporation, an American energy company based in Houston, Texas, and the de facto dissolution of Arthur Andersen, which was one of the five largest audit and accountancy partnerships in the world. In addition to being the largest bankruptcy reorganization in American history at that time, Enron was cited as the biggest audit failure.[1]:61Enron was formed in 1985 by Kenneth Lay after merging Houston Natural Gas and InterNorth. Several years later, when Jeffrey Skilling was hired, he developed a staff of executives that – by the use of accounting loopholes, special purpose entities, and poor financial reporting – were able to hide billions of dollars in debt from failed deals and projects. Chief Financial Officer Andrew Fastow and other executives not only misled Enron's Board of Directors and Audit Committee on high-risk accounting practices, but also pressured Arthur Andersen to ignore the issues.Enron shareholders filed a $40 billion lawsuit after the company's stock price, which achieved a high of US$90.75 per share in mid-2000, plummeted to less than $1 by the end of November 2001.[2] The U.S. Securities and Exchange Commission (SEC) began an investigation, and rival Houston competitor Dynegy offered to purchase the company at a very low price. The deal failed, and on December 2, 2001, Enron filed for bankruptcy under Chapter 11 of the United States Bankruptcy Code. Enron's $63.4 billion in assets made it the largest corporate bankruptcy in U.S. history until WorldCom's bankruptcy the next year.[3]Many executives at Enron were indicted for a variety of charges and some were later sentenced to prison. Andersen was found guilty of illegally destroying documents relevant to the SEC investigation, which voided its license to audit public companies and effectively closed the firm. By the time the ruling was overturned at the U.S. Supreme Court, the company had lost the majority of its customers and had ceased operating. Enron employees and shareholders received limited returns in lawsuits, despite losing billions in pensions and stock prices.As a consequence of the scandal, new regulations and legislation were enacted to expand the accuracy of financial reporting for public companies.[4] One piece of legislation, the Sarbanes–Oxley Act, increased penalties for destroying, altering, or fabricating records in federal investigations or for attempting to defraud shareholders.[5] The act also increased the accountability of auditing firms to remain unbiased and independent of their clients.[4]\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('test2.txt', 'r') as file:\n",
    "    text2 = file.read().replace('\\n', '')\n",
    "text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5oeB0aa-21pn"
   },
   "outputs": [],
   "source": [
    "def string_to_kmer_set(Astr, k):\n",
    "    return set([Astr[i:i+k] for i in range(len(Astr)-k+1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "zBg7yRsR21qB",
    "outputId": "5f25d114-052d-48ef-f60e-1a67d3760d2b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' \"',\n",
       " ' (',\n",
       " ' 0',\n",
       " ' 1',\n",
       " ' 2',\n",
       " ' 4',\n",
       " ' 7',\n",
       " ' A',\n",
       " ' B',\n",
       " ' C',\n",
       " ' D',\n",
       " ' E',\n",
       " ' F',\n",
       " ' G',\n",
       " ' H',\n",
       " ' I',\n",
       " ' J',\n",
       " ' K',\n",
       " ' L',\n",
       " ' M',\n",
       " ' O',\n",
       " ' P',\n",
       " ' R',\n",
       " ' S',\n",
       " ' T',\n",
       " ' U',\n",
       " ' V',\n",
       " ' W',\n",
       " ' a',\n",
       " ' b',\n",
       " ' c',\n",
       " ' d',\n",
       " ' e',\n",
       " ' f',\n",
       " ' g',\n",
       " ' h',\n",
       " ' i',\n",
       " ' j',\n",
       " ' k',\n",
       " ' l',\n",
       " ' m',\n",
       " ' n',\n",
       " ' o',\n",
       " ' p',\n",
       " ' q',\n",
       " ' r',\n",
       " ' s',\n",
       " ' t',\n",
       " ' u',\n",
       " ' v',\n",
       " ' w',\n",
       " ' y',\n",
       " '\" ',\n",
       " '\")',\n",
       " '\",',\n",
       " '\".',\n",
       " '\"C',\n",
       " '\"D',\n",
       " '\"M',\n",
       " '\"a',\n",
       " '\"r',\n",
       " '\"u',\n",
       " \"'s\",\n",
       " \"'t\",\n",
       " \"'v\",\n",
       " '(A',\n",
       " '(a',\n",
       " '(i',\n",
       " '(n',\n",
       " '(o',\n",
       " ') ',\n",
       " ')*',\n",
       " ').',\n",
       " '* ',\n",
       " ', ',\n",
       " '-E',\n",
       " '-a',\n",
       " '-l',\n",
       " '-m',\n",
       " '-n',\n",
       " '. ',\n",
       " '.\"',\n",
       " '.)',\n",
       " '.*',\n",
       " '.,',\n",
       " '.5',\n",
       " '.I',\n",
       " '.T',\n",
       " '.c',\n",
       " '.e',\n",
       " '/o',\n",
       " '0 ',\n",
       " '0.',\n",
       " '00',\n",
       " '01',\n",
       " '04',\n",
       " '09',\n",
       " '1 ',\n",
       " '1,',\n",
       " '11',\n",
       " '12',\n",
       " '15',\n",
       " '2 ',\n",
       " '2,',\n",
       " '20',\n",
       " '21',\n",
       " '23',\n",
       " '3M',\n",
       " '4 ',\n",
       " '42',\n",
       " '5 ',\n",
       " '50',\n",
       " '5M',\n",
       " '7,',\n",
       " '9 ',\n",
       " ': ',\n",
       " '; ',\n",
       " '@e',\n",
       " 'A ',\n",
       " 'AA',\n",
       " 'AI',\n",
       " 'AL',\n",
       " 'AS',\n",
       " 'Al',\n",
       " 'Am',\n",
       " 'An',\n",
       " 'Ap',\n",
       " 'As',\n",
       " 'Au',\n",
       " 'B,',\n",
       " 'Be',\n",
       " 'C ',\n",
       " 'CA',\n",
       " 'CB',\n",
       " 'CE',\n",
       " 'Ch',\n",
       " 'Co',\n",
       " 'Cr',\n",
       " 'D5',\n",
       " 'DR',\n",
       " 'Da',\n",
       " 'Do',\n",
       " 'EA',\n",
       " 'ED',\n",
       " 'Em',\n",
       " 'En',\n",
       " 'Fa',\n",
       " 'Fe',\n",
       " 'Ge',\n",
       " 'Hu',\n",
       " 'I ',\n",
       " \"I'\",\n",
       " 'I,',\n",
       " 'IM',\n",
       " 'IT',\n",
       " 'If',\n",
       " 'In',\n",
       " 'It',\n",
       " 'Ji',\n",
       " 'Jo',\n",
       " 'K.',\n",
       " 'Ka',\n",
       " 'Ki',\n",
       " 'LO',\n",
       " 'Le',\n",
       " 'Li',\n",
       " 'M ',\n",
       " 'M.',\n",
       " 'MD',\n",
       " 'MI',\n",
       " 'MS',\n",
       " 'Ma',\n",
       " 'Mb',\n",
       " 'Me',\n",
       " 'O ',\n",
       " 'Or',\n",
       " 'Ox',\n",
       " 'Pa',\n",
       " 'Pe',\n",
       " 'Pr',\n",
       " 'RI',\n",
       " 'RM',\n",
       " 'Re',\n",
       " 'Ro',\n",
       " 'S ',\n",
       " 'S,',\n",
       " 'SI',\n",
       " 'SR',\n",
       " 'Sh',\n",
       " 'Sm',\n",
       " 'So',\n",
       " 'Sp',\n",
       " 'Sy',\n",
       " 'T,',\n",
       " 'Th',\n",
       " 'UC',\n",
       " 'Un',\n",
       " 'Ve',\n",
       " 'Wi',\n",
       " 'Wo',\n",
       " '_a',\n",
       " 'a ',\n",
       " 'a,',\n",
       " 'a-',\n",
       " 'ab',\n",
       " 'ac',\n",
       " 'ad',\n",
       " 'ae',\n",
       " 'af',\n",
       " 'ag',\n",
       " 'ai',\n",
       " 'ak',\n",
       " 'al',\n",
       " 'am',\n",
       " 'an',\n",
       " 'ap',\n",
       " 'ar',\n",
       " 'as',\n",
       " 'at',\n",
       " 'au',\n",
       " 'av',\n",
       " 'ay',\n",
       " 'b,',\n",
       " 'ba',\n",
       " 'be',\n",
       " 'bi',\n",
       " 'bl',\n",
       " 'bo',\n",
       " 'bs',\n",
       " 'bu',\n",
       " 'by',\n",
       " 'c ',\n",
       " 'c\"',\n",
       " 'c,',\n",
       " 'c.',\n",
       " 'ca',\n",
       " 'ce',\n",
       " 'ch',\n",
       " 'ci',\n",
       " 'cl',\n",
       " 'co',\n",
       " 'cr',\n",
       " 'ct',\n",
       " 'cu',\n",
       " 'cy',\n",
       " 'd ',\n",
       " 'd)',\n",
       " 'd*',\n",
       " 'd.',\n",
       " 'd/',\n",
       " 'da',\n",
       " 'dd',\n",
       " 'de',\n",
       " 'dg',\n",
       " 'di',\n",
       " 'do',\n",
       " 'dr',\n",
       " 'ds',\n",
       " 'du',\n",
       " 'e ',\n",
       " 'e)',\n",
       " 'e,',\n",
       " 'e-',\n",
       " 'e.',\n",
       " 'e;',\n",
       " 'ea',\n",
       " 'eb',\n",
       " 'ec',\n",
       " 'ed',\n",
       " 'ee',\n",
       " 'ef',\n",
       " 'eg',\n",
       " 'ei',\n",
       " 'ek',\n",
       " 'el',\n",
       " 'em',\n",
       " 'en',\n",
       " 'eo',\n",
       " 'ep',\n",
       " 'eq',\n",
       " 'er',\n",
       " 'es',\n",
       " 'et',\n",
       " 'ev',\n",
       " 'ew',\n",
       " 'ex',\n",
       " 'ey',\n",
       " 'f ',\n",
       " 'fe',\n",
       " 'ff',\n",
       " 'fi',\n",
       " 'fo',\n",
       " 'fr',\n",
       " 'ft',\n",
       " 'fu',\n",
       " 'g ',\n",
       " 'ga',\n",
       " 'ge',\n",
       " 'gh',\n",
       " 'gi',\n",
       " 'gn',\n",
       " 'gr',\n",
       " 'gs',\n",
       " 'gt',\n",
       " 'gu',\n",
       " 'gy',\n",
       " 'gz',\n",
       " 'h ',\n",
       " 'h\"',\n",
       " 'h,',\n",
       " 'h.',\n",
       " 'ha',\n",
       " 'he',\n",
       " 'hi',\n",
       " 'hm',\n",
       " 'hn',\n",
       " 'ho',\n",
       " 'ht',\n",
       " 'hu',\n",
       " 'i.',\n",
       " 'ia',\n",
       " 'ib',\n",
       " 'ic',\n",
       " 'id',\n",
       " 'ie',\n",
       " 'if',\n",
       " 'ig',\n",
       " 'ik',\n",
       " 'il',\n",
       " 'im',\n",
       " 'in',\n",
       " 'io',\n",
       " 'ip',\n",
       " 'ir',\n",
       " 'is',\n",
       " 'it',\n",
       " 'iu',\n",
       " 'iv',\n",
       " 'iz',\n",
       " 'je',\n",
       " 'ju',\n",
       " 'k ',\n",
       " 'k,',\n",
       " 'k-',\n",
       " 'ke',\n",
       " 'kk',\n",
       " 'kn',\n",
       " 'ks',\n",
       " 'kw',\n",
       " 'l ',\n",
       " 'l\"',\n",
       " 'l-',\n",
       " 'l.',\n",
       " 'la',\n",
       " 'lb',\n",
       " 'ld',\n",
       " 'le',\n",
       " 'li',\n",
       " 'lk',\n",
       " 'll',\n",
       " 'lo',\n",
       " 'ls',\n",
       " 'lt',\n",
       " 'lu',\n",
       " 'lv',\n",
       " 'ly',\n",
       " 'm ',\n",
       " 'm,',\n",
       " 'mR',\n",
       " 'ma',\n",
       " 'mb',\n",
       " 'me',\n",
       " 'mi',\n",
       " 'mm',\n",
       " 'mo',\n",
       " 'mp',\n",
       " 'ms',\n",
       " 'my',\n",
       " 'n ',\n",
       " 'n\"',\n",
       " \"n'\",\n",
       " 'n,',\n",
       " 'n-',\n",
       " 'n.',\n",
       " 'n:',\n",
       " 'n;',\n",
       " 'na',\n",
       " 'nc',\n",
       " 'nd',\n",
       " 'ne',\n",
       " 'nf',\n",
       " 'ng',\n",
       " 'ni',\n",
       " 'nk',\n",
       " 'nl',\n",
       " 'nn',\n",
       " 'no',\n",
       " 'nr',\n",
       " 'ns',\n",
       " 'nt',\n",
       " 'nu',\n",
       " 'nv',\n",
       " 'ny',\n",
       " 'o ',\n",
       " 'o,',\n",
       " 'o_',\n",
       " 'ob',\n",
       " 'oc',\n",
       " 'od',\n",
       " 'oe',\n",
       " 'of',\n",
       " 'og',\n",
       " 'oh',\n",
       " 'oi',\n",
       " 'oj',\n",
       " 'ol',\n",
       " 'om',\n",
       " 'on',\n",
       " 'oo',\n",
       " 'op',\n",
       " 'or',\n",
       " 'os',\n",
       " 'ot',\n",
       " 'ou',\n",
       " 'ov',\n",
       " 'ow',\n",
       " 'oy',\n",
       " 'p ',\n",
       " 'pa',\n",
       " 'pe',\n",
       " 'pi',\n",
       " 'pl',\n",
       " 'po',\n",
       " 'pp',\n",
       " 'pr',\n",
       " 'pt',\n",
       " 'pu',\n",
       " 'py',\n",
       " 'qu',\n",
       " 'r ',\n",
       " 'r)',\n",
       " 'r,',\n",
       " 'r@',\n",
       " 'ra',\n",
       " 'rc',\n",
       " 'rd',\n",
       " 're',\n",
       " 'rg',\n",
       " 'ri',\n",
       " 'rk',\n",
       " 'rl',\n",
       " 'rm',\n",
       " 'rn',\n",
       " 'ro',\n",
       " 'rp',\n",
       " 'rr',\n",
       " 'rs',\n",
       " 'rt',\n",
       " 'rv',\n",
       " 'ry',\n",
       " 's ',\n",
       " 's\"',\n",
       " 's)',\n",
       " 's*',\n",
       " 's,',\n",
       " 's.',\n",
       " 's@',\n",
       " 'sa',\n",
       " 'sc',\n",
       " 'sd',\n",
       " 'se',\n",
       " 'sh',\n",
       " 'si',\n",
       " 'sk',\n",
       " 'sl',\n",
       " 'so',\n",
       " 'sp',\n",
       " 'ss',\n",
       " 'st',\n",
       " 'su',\n",
       " 'sw',\n",
       " 'sy',\n",
       " 't ',\n",
       " 't,',\n",
       " 't.',\n",
       " 'tT',\n",
       " 'ta',\n",
       " 'tc',\n",
       " 'te',\n",
       " 'th',\n",
       " 'ti',\n",
       " 'tl',\n",
       " 'to',\n",
       " 'tr',\n",
       " 'ts',\n",
       " 'tt',\n",
       " 'tu',\n",
       " 'tw',\n",
       " 'ty',\n",
       " 'u ',\n",
       " 'u,',\n",
       " 'ua',\n",
       " 'ub',\n",
       " 'uc',\n",
       " 'ud',\n",
       " 'ue',\n",
       " 'ug',\n",
       " 'ui',\n",
       " 'ul',\n",
       " 'um',\n",
       " 'un',\n",
       " 'up',\n",
       " 'ur',\n",
       " 'us',\n",
       " 'ut',\n",
       " 'va',\n",
       " 've',\n",
       " 'vi',\n",
       " 'vo',\n",
       " 'vs',\n",
       " 'w ',\n",
       " 'w,',\n",
       " 'w.',\n",
       " 'wa',\n",
       " 'wd',\n",
       " 'we',\n",
       " 'wh',\n",
       " 'wi',\n",
       " 'wl',\n",
       " 'wo',\n",
       " 'ww',\n",
       " 'xa',\n",
       " 'xb',\n",
       " 'xp',\n",
       " 'y ',\n",
       " 'y,',\n",
       " 'y.',\n",
       " 'ye',\n",
       " 'yl',\n",
       " 'ym',\n",
       " 'yo',\n",
       " 'ys',\n",
       " 'za',\n",
       " 'ze',\n",
       " 'zi'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_to_kmer_set(text1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' $',\n",
       " ' (',\n",
       " ' 1',\n",
       " ' 2',\n",
       " ' A',\n",
       " ' B',\n",
       " ' C',\n",
       " ' D',\n",
       " ' E',\n",
       " ' F',\n",
       " ' G',\n",
       " ' H',\n",
       " ' I',\n",
       " ' J',\n",
       " ' K',\n",
       " ' L',\n",
       " ' N',\n",
       " ' O',\n",
       " ' S',\n",
       " ' T',\n",
       " ' U',\n",
       " ' W',\n",
       " ' a',\n",
       " ' b',\n",
       " ' c',\n",
       " ' d',\n",
       " ' e',\n",
       " ' f',\n",
       " ' g',\n",
       " ' h',\n",
       " ' i',\n",
       " ' l',\n",
       " ' m',\n",
       " ' n',\n",
       " ' o',\n",
       " ' p',\n",
       " ' r',\n",
       " ' s',\n",
       " ' t',\n",
       " ' u',\n",
       " ' v',\n",
       " ' w',\n",
       " ' y',\n",
       " ' –',\n",
       " '$1',\n",
       " '$4',\n",
       " '$6',\n",
       " '$9',\n",
       " \"'s\",\n",
       " '(S',\n",
       " ') ',\n",
       " ', ',\n",
       " '-2',\n",
       " '-r',\n",
       " '. ',\n",
       " '.4',\n",
       " '.7',\n",
       " '.A',\n",
       " '.E',\n",
       " '.S',\n",
       " '.[',\n",
       " '0 ',\n",
       " '0,',\n",
       " '0.',\n",
       " '00',\n",
       " '01',\n",
       " '1 ',\n",
       " '1,',\n",
       " '1.',\n",
       " '11',\n",
       " '19',\n",
       " '1E',\n",
       " '1]',\n",
       " '2,',\n",
       " '20',\n",
       " '2]',\n",
       " '3.',\n",
       " '3]',\n",
       " '4 ',\n",
       " '40',\n",
       " '4]',\n",
       " '5 ',\n",
       " '5]',\n",
       " '61',\n",
       " '63',\n",
       " '75',\n",
       " '85',\n",
       " '90',\n",
       " '98',\n",
       " ':6',\n",
       " 'Ac',\n",
       " 'Am',\n",
       " 'An',\n",
       " 'Ar',\n",
       " 'As',\n",
       " 'Au',\n",
       " 'Ba',\n",
       " 'Bo',\n",
       " 'By',\n",
       " 'C ',\n",
       " 'C)',\n",
       " 'Ch',\n",
       " 'Co',\n",
       " 'De',\n",
       " 'Di',\n",
       " 'Dy',\n",
       " 'EC',\n",
       " 'En',\n",
       " 'Ex',\n",
       " 'Fa',\n",
       " 'Fi',\n",
       " 'Ga',\n",
       " 'Ho',\n",
       " 'In',\n",
       " 'Je',\n",
       " 'Ke',\n",
       " 'La',\n",
       " 'Ma',\n",
       " 'Na',\n",
       " 'No',\n",
       " 'Oc',\n",
       " 'Of',\n",
       " 'On',\n",
       " 'Ox',\n",
       " 'S$',\n",
       " 'S.',\n",
       " 'SE',\n",
       " 'Sa',\n",
       " 'Se',\n",
       " 'Sk',\n",
       " 'St',\n",
       " 'Su',\n",
       " 'Te',\n",
       " 'Th',\n",
       " 'U.',\n",
       " 'US',\n",
       " 'Un',\n",
       " 'Wo',\n",
       " '[1',\n",
       " '[2',\n",
       " '[3',\n",
       " '[4',\n",
       " '[5',\n",
       " '] ',\n",
       " ']:',\n",
       " ']M',\n",
       " 'a ',\n",
       " 'ab',\n",
       " 'ac',\n",
       " 'ad',\n",
       " 'af',\n",
       " 'ai',\n",
       " 'aj',\n",
       " 'al',\n",
       " 'an',\n",
       " 'ap',\n",
       " 'ar',\n",
       " 'as',\n",
       " 'at',\n",
       " 'au',\n",
       " 'aw',\n",
       " 'ay',\n",
       " 'ba',\n",
       " 'be',\n",
       " 'bi',\n",
       " 'bl',\n",
       " 'br',\n",
       " 'bt',\n",
       " 'bu',\n",
       " 'by',\n",
       " 'c ',\n",
       " 'ca',\n",
       " 'cc',\n",
       " 'ce',\n",
       " 'ch',\n",
       " 'ci',\n",
       " 'ck',\n",
       " 'cl',\n",
       " 'co',\n",
       " 'cr',\n",
       " 'ct',\n",
       " 'cu',\n",
       " 'cy',\n",
       " 'd ',\n",
       " 'd,',\n",
       " 'd-',\n",
       " 'd.',\n",
       " 'dC',\n",
       " 'da',\n",
       " 'dd',\n",
       " 'de',\n",
       " 'di',\n",
       " 'do',\n",
       " 'dr',\n",
       " 'ds',\n",
       " 'e ',\n",
       " 'e,',\n",
       " 'e.',\n",
       " 'ea',\n",
       " 'eb',\n",
       " 'ec',\n",
       " 'ed',\n",
       " 'ee',\n",
       " 'ef',\n",
       " 'eg',\n",
       " 'eh',\n",
       " 'ei',\n",
       " 'el',\n",
       " 'em',\n",
       " 'en',\n",
       " 'eo',\n",
       " 'ep',\n",
       " 'eq',\n",
       " 'er',\n",
       " 'es',\n",
       " 'et',\n",
       " 'ev',\n",
       " 'ew',\n",
       " 'ex',\n",
       " 'ey',\n",
       " 'f ',\n",
       " 'fa',\n",
       " 'fe',\n",
       " 'ff',\n",
       " 'fi',\n",
       " 'fo',\n",
       " 'fr',\n",
       " 'ft',\n",
       " 'g ',\n",
       " 'g,',\n",
       " 'g.',\n",
       " 'ga',\n",
       " 'ge',\n",
       " 'gg',\n",
       " 'gh',\n",
       " 'gi',\n",
       " 'gn',\n",
       " 'gu',\n",
       " 'gy',\n",
       " 'h ',\n",
       " 'h-',\n",
       " 'h.',\n",
       " 'ha',\n",
       " 'he',\n",
       " 'hi',\n",
       " 'ho',\n",
       " 'hu',\n",
       " 'ia',\n",
       " 'ic',\n",
       " 'id',\n",
       " 'ie',\n",
       " 'ig',\n",
       " 'il',\n",
       " 'im',\n",
       " 'in',\n",
       " 'io',\n",
       " 'ip',\n",
       " 'ir',\n",
       " 'is',\n",
       " 'it',\n",
       " 'iv',\n",
       " 'iz',\n",
       " 'je',\n",
       " 'jo',\n",
       " 'k ',\n",
       " 'ki',\n",
       " 'kr',\n",
       " 'l ',\n",
       " 'l,',\n",
       " 'lT',\n",
       " 'la',\n",
       " 'ld',\n",
       " 'le',\n",
       " 'li',\n",
       " 'll',\n",
       " 'lo',\n",
       " 'ls',\n",
       " 'lt',\n",
       " 'lu',\n",
       " 'ly',\n",
       " 'm ',\n",
       " \"m'\",\n",
       " 'm.',\n",
       " 'ma',\n",
       " 'mb',\n",
       " 'me',\n",
       " 'mi',\n",
       " 'mm',\n",
       " 'mp',\n",
       " 'ms',\n",
       " 'n ',\n",
       " \"n'\",\n",
       " 'n,',\n",
       " 'n.',\n",
       " 'na',\n",
       " 'nb',\n",
       " 'nc',\n",
       " 'nd',\n",
       " 'ne',\n",
       " 'ng',\n",
       " 'ni',\n",
       " 'nk',\n",
       " 'nl',\n",
       " 'nn',\n",
       " 'no',\n",
       " 'nr',\n",
       " 'ns',\n",
       " 'nt',\n",
       " 'nv',\n",
       " 'ny',\n",
       " 'o ',\n",
       " 'oa',\n",
       " 'ob',\n",
       " 'oc',\n",
       " 'od',\n",
       " 'of',\n",
       " 'oi',\n",
       " 'oj',\n",
       " 'ol',\n",
       " 'om',\n",
       " 'on',\n",
       " 'oo',\n",
       " 'op',\n",
       " 'or',\n",
       " 'os',\n",
       " 'ot',\n",
       " 'ou',\n",
       " 'ov',\n",
       " 'ow',\n",
       " 'oy',\n",
       " 'pa',\n",
       " 'pe',\n",
       " 'ph',\n",
       " 'pi',\n",
       " 'pl',\n",
       " 'po',\n",
       " 'pr',\n",
       " 'ps',\n",
       " 'pt',\n",
       " 'pu',\n",
       " 'qu',\n",
       " 'r ',\n",
       " 'r,',\n",
       " 'r.',\n",
       " 'rN',\n",
       " 'ra',\n",
       " 'rb',\n",
       " 'rc',\n",
       " 'rd',\n",
       " 're',\n",
       " 'rg',\n",
       " 'ri',\n",
       " 'rl',\n",
       " 'rm',\n",
       " 'rn',\n",
       " 'ro',\n",
       " 'rp',\n",
       " 'rs',\n",
       " 'rt',\n",
       " 'ru',\n",
       " 'ry',\n",
       " 's ',\n",
       " 's,',\n",
       " 's.',\n",
       " 'sc',\n",
       " 'se',\n",
       " 'sh',\n",
       " 'si',\n",
       " 'sk',\n",
       " 'sl',\n",
       " 'so',\n",
       " 'sp',\n",
       " 'ss',\n",
       " 'st',\n",
       " 'su',\n",
       " 's–',\n",
       " 't ',\n",
       " 't,',\n",
       " 'ta',\n",
       " 'tc',\n",
       " 'te',\n",
       " 'th',\n",
       " 'ti',\n",
       " 'tn',\n",
       " 'to',\n",
       " 'tr',\n",
       " 'ts',\n",
       " 'tt',\n",
       " 'tu',\n",
       " 'ty',\n",
       " 'ua',\n",
       " 'ub',\n",
       " 'ud',\n",
       " 'ue',\n",
       " 'ui',\n",
       " 'ul',\n",
       " 'um',\n",
       " 'un',\n",
       " 'up',\n",
       " 'ur',\n",
       " 'us',\n",
       " 'ut',\n",
       " 'va',\n",
       " 've',\n",
       " 'vo',\n",
       " 'w ',\n",
       " 'wa',\n",
       " 'we',\n",
       " 'wh',\n",
       " 'wo',\n",
       " 'ws',\n",
       " 'xa',\n",
       " 'xc',\n",
       " 'xe',\n",
       " 'xl',\n",
       " 'xp',\n",
       " 'xt',\n",
       " 'y ',\n",
       " \"y'\",\n",
       " 'ye',\n",
       " 'yi',\n",
       " 'yn',\n",
       " 'za',\n",
       " 'ze',\n",
       " '– ',\n",
       " '–O'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_to_kmer_set(text2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f0gY_lhQ21qc"
   },
   "source": [
    "The Jaccard similarity coefficient $J(A, B)$ of non-empty sets $A$ and $B$ is:\n",
    "\n",
    "$$\\frac{|A \\cap B|}{|A \\cup B|}$$\n",
    "\n",
    "It equals 1 when the sets are identical and 0 when they are disjoint.  Otherwise it is between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_SptkYC021qg"
   },
   "outputs": [],
   "source": [
    "def jaccard(Aset, Bset):\n",
    "    # return Jaccard similarity coefficient between two sets\n",
    "    isz = len(Aset.intersection(Bset))\n",
    "    return float(isz) / (len(Aset) + len(Bset) - isz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5vUjMq2_21qq"
   },
   "outputs": [],
   "source": [
    "def jaccard_kmer(Astr, Bstr, k):\n",
    "    # turn two strings into sets, then return Jaccard similarity coefficient of those sets\n",
    "    return jaccard(string_to_kmer_set(Astr, k),\n",
    "                   string_to_kmer_set(Bstr, k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ep80YUP521qx",
    "outputId": "1cf85129-e628-46b0-8e4c-7647b876e5c9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46676737160120846"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaccard_kmer(text1, text2, 2)\n",
    "# intersection: {AB}, union: {AB, BC, BD}\n",
    "# so answer = 1/3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L5tgDr1H21q-"
   },
   "source": [
    "#### Evaluating use of sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5t8F6EDj21rA"
   },
   "source": [
    "Explicitly building and intersecting sets of strings seems inefficient.  Let's see how long it takes to run on many randomly generated pairs of similar strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i1igihRz21rC"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def add_mutations(string, num_muts):\n",
    "    \"\"\" Add num_muts random substitution mutations to string \"\"\"\n",
    "    for _ in range(num_muts):\n",
    "        rndi = random.randint(0, len(string)-1)\n",
    "        string = string[:rndi] + random.choice('ACGT') + string[rndi+1:]\n",
    "    return string\n",
    "\n",
    "def random_jaccard_kmer(length, k):\n",
    "    \"\"\" Make a random string and a second string separated from the\n",
    "        first by a few mutations, then return the two strings and\n",
    "        their jaccard similarity coefficient. \"\"\"\n",
    "    str1 = ''.join([random.choice('ACGT') for _ in range(length)])\n",
    "    str2 = add_mutations(str1, random.randint(0, float(length)/20))\n",
    "    return str1, str2, jaccard_kmer(str1, str2, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EMtVBEuo21rK",
    "outputId": "29ee30b3-5958-4dd3-f8fa-2cfe982a671b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ACGCGCGT'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(77)\n",
    "add_mutations('ACGTACGT', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_JV46GGP21rT",
    "outputId": "f56e02d1-a216-47b6-e7c0-7f32c2e45d38"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('GTTCGATCGGTTCAGGCGAA', 'GTTCGATCGGTTCAGGCGTA', 0.7777777777777778)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(76)\n",
    "random_jaccard_kmer(20, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7XVfV1iV21rd",
    "outputId": "b1206269-c225-43a4-a46a-571bf6e28d2e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35.53987351785223"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import timeit\n",
    "timeit.timeit('random_jaccard_kmer(1000, 10)',\n",
    "              setup='''\n",
    "from __main__ import random_jaccard_kmer;\n",
    "import random;\n",
    "random.seed(223)''',\n",
    "              number=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6v201FmL21rm"
   },
   "source": [
    "It takes >10 seconds to find Jaccard similarities between 10,000 random pairs of 100-long DNA strings, using $k$-mer length of 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NeY2gvyT21rr"
   },
   "source": [
    "### Min hashing\n",
    "\n",
    "#### Introduction\n",
    "\n",
    "How about: instead of using the set of all $k$-mers from each string, we pick one representative $k$-mer from each string.  Let's pick the *minimum alphabetically*.  For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yydzChdU21rt"
   },
   "outputs": [],
   "source": [
    "def string_to_min_kmer(Astr, k):\n",
    "    return min([Astr[i:i+k] for i in range(len(Astr)-k+1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gKMwldC_21r1",
    "outputId": "3c0a6ac4-dfab-4b6e-c7ca-e68960d9a30a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \"'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_to_min_kmer(text1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' $'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_to_min_kmer(text2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Oiyb05Mu21r9"
   },
   "source": [
    "We can compare two strings by comparing their minimal $k$-mers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7I-TUpe921sA"
   },
   "outputs": [],
   "source": [
    "def jaccard_min_kmer(Astr, Bstr, k):\n",
    "    return 1 if string_to_min_kmer(Astr, k) == string_to_min_kmer(Bstr, k) else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ETcVLkGK21sJ",
    "outputId": "33937f3e-2895-48b2-adbe-71adde95e3fa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaccard_min_kmer(text1, text2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KEXZjBPI21sT",
    "outputId": "6f8e58f6-3ce1-4be3-cf7c-3687ebf49663"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaccard_min_kmer(text1, text2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IhZbcu-Y21sd",
    "outputId": "ee5e8e24-d745-407b-9cd6-b9583d23d046"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaccard_min_kmer(text1, text2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Me2ia8mJ21si"
   },
   "source": [
    "This can yield a Jaccard similarity of 0 or 1; we cannot distinguish intermedaite amounts of similarity.  On the other hand, we avoided building any sets.\n",
    "\n",
    "#### Adding a hash function\n",
    "\n",
    "We'll use the [mmh3 library], which contains an implementation of [MurmurHash3], a fast and widely used non-cryptographic hash function.  Instead of taking our representative as being the minimal $k$-mer, we'll first *hash* the $k$-mers, then take the $k$-mer with minimal *hash value*:\n",
    "\n",
    "[MurmurHash3]: https://code.google.com/p/smhasher/wiki/MurmurHash3\n",
    "[mmh3 library]: https://pypi.python.org/pypi/mmh3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Ap2JpUY21sk"
   },
   "outputs": [],
   "source": [
    "# you might need to 'pip install mmh3' first\n",
    "import mmh3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hWtvn4qa21sp"
   },
   "outputs": [],
   "source": [
    "def string_to_min_hash(Astr, k):\n",
    "    return min([mmh3.hash (Astr[i:i+k]) for i in range(len(Astr)-k+1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eFdtvbV_21sv",
    "outputId": "3f854631-4be3-4bc9-9de7-3a9d344d3889"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2131080486"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_to_min_hash(text1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2131080486"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_to_min_hash(text2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3Sh8HRrN21s5"
   },
   "source": [
    "That's the minimum among the hash values of the 3-mers of \"hello\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H9CtmdhS21s-"
   },
   "outputs": [],
   "source": [
    "def jaccard_min_kmer_hash(Astr, Bstr, k):\n",
    "    return 1 if string_to_min_hash(Astr, k) == string_to_min_hash(Bstr, k) else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kyze9sqK21tJ",
    "outputId": "23321845-8b41-4629-d04c-b3632b204a55"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaccard_min_kmer_hash(text1, text2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R2mJBkkU21tO",
    "outputId": "b5e5931c-3b22-401e-cff4-3267a91fb9e8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaccard_min_kmer_hash(text1, text2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x0Cv2OAB21tT",
    "outputId": "17e3b33c-ef56-47e9-913e-aec64741cb18"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaccard_min_kmer_hash(text1, text2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h5qg5i-i21tX"
   },
   "source": [
    "`jaccard_min_kmer_hash`'s return value won't necessarily match `jaccard_min_kmer`'s, since the function permutes the alphabetical order of the $k$-mers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SiXe5VpB21tY"
   },
   "source": [
    "#### Multiple hash functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BYreDBto21tZ"
   },
   "source": [
    "`jaccard_min_kmer` and `jaccard_min_kmer_hash` return 0 or 1 -- not very precise!  We can get a better estimate by calling `jaccard_min_kmer_hash` multiple times, each time using a different hash function.\n",
    "\n",
    "Let's rewrite `string_to_min_hash` to include a `seed` parameter that [\"salts\"] the hash function.\n",
    "\n",
    "[\"salts\"]: http://en.wikipedia.org/wiki/Salt_(cryptography)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R6TBRUmc21ta"
   },
   "outputs": [],
   "source": [
    "def string_to_min_hash(Astr, k, seed=0):\n",
    "    return min([mmh3.hash(Astr[i:i+k], seed) for i in range(len(Astr)-k+1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kD4C4QBz21td"
   },
   "source": [
    "Now we can call `string_to_min_hash` with various hash functions, or various saltings of the same function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mIj80e2U21tf",
    "outputId": "fa9c2c11-9175-4a72-c5c9-c32c21852a64"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-2136386649,\n",
       " -2145369055,\n",
       " -2142325664,\n",
       " -2145778073,\n",
       " -2147403584,\n",
       " -2145381313,\n",
       " -2147466663,\n",
       " -2145060192,\n",
       " -2139187557,\n",
       " -2146623530]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[string_to_min_hash(text1, 3, k) for k in range(10, 20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-2140125177,\n",
       " -2141675998,\n",
       " -2147324818,\n",
       " -2146318056,\n",
       " -2147403584,\n",
       " -2146218578,\n",
       " -2147131694,\n",
       " -2145060192,\n",
       " -2144431913,\n",
       " -2143530132]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[string_to_min_hash(text2, 3, k) for k in range(10, 20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_enY1IFA21th"
   },
   "outputs": [],
   "source": [
    "def jaccard_min_kmer_hashes(Astr, Bstr, k, seeds=[0]):\n",
    "    tot = sum(string_to_min_hash(Astr, k, seed) == string_to_min_hash(Bstr, k, seed) for seed in seeds)\n",
    "    return float(tot) / len(seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9YU9BkeD21tj",
    "outputId": "dad5b7b5-5935-4725-befa-423417d31186"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaccard_min_kmer_hashes(text1, text2, 2, seeds=range(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x-Gm8wbK21tl"
   },
   "source": [
    "Not a terrible estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jjxzbTG621tm",
    "outputId": "7bee9e6c-baed-4bae-a24a-41e47232b6e9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaccard_min_kmer_hashes(text1, text2, 2, seeds=range(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iusvp8JZ21tp"
   },
   "source": [
    "Again, not terrible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JFip-N0G21tp",
    "outputId": "5f062792-e6e1-40a4-9eee-7c36ee5b91b2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4604"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaccard_min_kmer_hashes(text1, text2, 2, seeds=range(10000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LyvZPo4c21ts"
   },
   "source": [
    "A very good estimate: off by only about 1%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "URUMZV4h21ts"
   },
   "source": [
    "Why does this function give an estimate of Jaccard similarity?  Each hash function *permutes* the ordering of the $k$-mers differently.  For each permutation, some $k$-mer from the union of all $k$-mers becomes the minimal one.  By calculating the fraction of the hash functions for which this minimal $k$-mer is present in both sets, we're estimating the size of the intersection divided by the size of the union: the Jaccard similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VzoJZgiZ21tt",
    "outputId": "2f49adce-301c-409f-9e2e-5f26a283dbdb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "184.4082271921136"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def random_jaccard_kmer(length, k):\n",
    "    str1 = ''.join([random.choice('ACGT') for _ in range(length)])\n",
    "    str2 = add_mutations(str1, random.randint(0, length/20))\n",
    "    return str1, str2, jaccard_min_kmer_hashes(str1, str2, k, seeds=range(10))\n",
    "\n",
    "import timeit\n",
    "timeit.timeit('random_jaccard_kmer(1000, 10)',\n",
    "              setup='''\n",
    "from __main__ import random_jaccard_kmer;\n",
    "import random;\n",
    "random.seed(223)''',\n",
    "              number=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3sBtmMDF21tw"
   },
   "source": [
    "It's slower than what we had before, but for large enough sets it could be faster."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Minhash.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
